# Copyright 2021 - 2022 IBM Corporation

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# submit this from ACM
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: fiveg-subnet-
spec:
  entrypoint: handlerequest
  arguments:
    parameters:
    - name: registry
      value: "OVERRIDE"

    - name: cluster

    - name: cluster_core
      value: "OVERRIDE"

    - name: smf_name
      value: "OVERRIDE"

    - name: sst
      value: "OVERRIDE"
    - name: sd
      value: "OVERRIDE"

    - name: pool
      value: "0.0.0.0/16"

    - name: networks

    - name: connectedFrom
    - name: group

    - name: network_name
      # OVERRIDE values implies not to create datanetwork
      value: "OVERRIDE"
    - name: network_master
      value: "OVERRIDE"
    - name: network_range
      value: "OVERRIDE"
    - name: network_start
      value: "OVERRIDE"
    - name: network_end
      value: "OVERRIDE"

    - name: core_namespace
      value: 5g-core

    - name: kafka_ip
      value: "OVERRIDE"
    - name: kafka_port
      value: 9092


  templates:
  - name: handlerequest
    dag:
      tasks:
      - name: create-subnet-deploy-subscription
        template: create-subnet-deploy-subscription

      - name: wait-for-subnet-status
        dependencies: [create-subnet-deploy-subscription]
        template: consume
        arguments:
          parameters:
          - name: kafka_ip
            value: "{{workflow.parameters.kafka_ip}}"
          - name: kafka_port
            value: "{{workflow.parameters.kafka_port}}"
          - name: kafka_topic
            value: "{{workflow.parameters.cluster}}"
          - name: msg_id
            value: "deploy-{{workflow.name}}"

      - name: create-subnet-configure-subscription
        dependencies: [wait-for-subnet-status]
        template: create-subnet-configure-subscription
        arguments:
          parameters:
          - name: upf_sbi
            value: "{{tasks.wait-for-subnet-status.outputs.parameters.sbi-ip}}"
          - name: upf_up
            value: "{{tasks.wait-for-subnet-status.outputs.parameters.up-ip}}"

  - name: create-subnet-deploy-subscription
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: apps.open-cluster-management.io/v1
        kind: Subscription
        metadata:
          annotations:
            apps.open-cluster-management.io/github-branch: multi-cluster-1
            apps.open-cluster-management.io/github-path: subnet-deploy-wf
          name: deploy-{{workflow.name}}
        spec:
          channel: gitops-chn-ns/gitops
          placement:
            clusters:
            - name: {{workflow.parameters.cluster}}
          packageOverrides:
          - packageName: fiveg-subnet-deploy
            packageOverrides:
            - path: metadata.name
              # also considered to be "msg_id"
              value: deploy-{{workflow.name}}
            - path: spec.arguments.parameters
              value:
              - name: registry
                value: {{workflow.parameters.registry}}

              - name: networks
                # management and useplane networks to get created at the edge
                value: |
                  {{workflow.parameters.networks}}

              - name: network_name
                value: "{{workflow.parameters.network_name}}"
              - name: network_master
                value: "{{workflow.parameters.network_master}}"
              - name: network_range
                value: "{{workflow.parameters.network_range}}"
              - name: network_start
                value: "{{workflow.parameters.network_start}}"
              - name: network_end
                value: "{{workflow.parameters.network_end}}"

              - name: pool
                value: "{{workflow.parameters.pool}}"
              - name: kafka_ip
                value: "{{workflow.parameters.kafka_ip}}"
              - name: kafka_port
                value: "{{workflow.parameters.kafka_port}}"

              - name: topic
                value: "{{workflow.parameters.cluster}}"

              - name: msg_id
                value: "deploy-{{workflow.name}}"

  - name: create-subnet-configure-subscription
    inputs:
      parameters:
      - name: upf_sbi
      - name: upf_up
    resource:
      action: create
      setOwnerReference: true
      manifest: |
        apiVersion: apps.open-cluster-management.io/v1
        kind: Subscription
        metadata:
          annotations:
            apps.open-cluster-management.io/github-branch: multi-cluster-1
            apps.open-cluster-management.io/github-path: subnet-configure-wf
          name: configure-{{workflow.name}}
        spec:
          channel: gitops-chn-ns/gitops
          placement:
            clusters:
            - name: {{workflow.parameters.cluster_core}}
          packageOverrides:
          - packageName: fiveg-subnet-configure
            packageOverrides:
            - path: metadata.name
              value: configure-{{workflow.name}}
            - path: spec.arguments.parameters
              value:
              - name: smf_name
                value: {{workflow.parameters.smf_name}}
              - name: core_namespace
                value: {{workflow.parameters.core_namespace}}
              - name: upf_sbi
                value: {{inputs.parameters.upf_sbi}}
              - name: upf_up
                value: {{inputs.parameters.upf_up}}
              - name: sst
                value: "{{workflow.parameters.sst}}"
              - name: sd
                value: "{{workflow.parameters.sd}}"

              - name: pool
                value: "{{workflow.parameters.pool}}"

              - name: connectedFrom
                value: "{{workflow.parameters.connectedFrom}}"
              
              - name: group
                value: "{{workflow.parameters.group}}"

  - name: delay-seconds
    inputs:
      parameters:
      - name: seconds
    container:
      image: alpine:3.7
      command: [sh, -c]
      args: ["sleep {{inputs.parameters.seconds}};"]

  - name: kubectl-jq-script
    # NOTE: in this version, subscription CR must exist already with its status being set
    inputs:
      parameters:
      - name: subscription_name
      - name: cluster_name
      - name: namespace
      - name: channel_name
      - name: package_name
      - name: cr_name
      - name: network_name
    script:
      image: "{{workflow.parameters.registry}}/argoproj/argoexec:v2.12.0-rc2"
      imagePullPolicy: IfNotPresent
      command: [sh]
      source: |
        kubectl get subscriptions.apps.open-cluster-management.io {{inputs.parameters.subscription_name}} -o json | jq '.status.statuses."{{inputs.parameters.cluster_name}}".packages."{{inputs.parameters.channel_name}}-{{inputs.parameters.cr_name}}-{{inputs.parameters.package_name}}".resourceStatus.outputs.parameters[0].value' | jq -r . | jq -r '.[] | select(.name=="{{inputs.parameters.namespace}}/{{inputs.parameters.network_name}}") | .ips[0]'

  - name: consume
    # Consumes a message from kafka broker
    #
    # Parameters
    # kafka_topic: the topic to publish the message on (string)
    # kafka_ip: ipaddress of the kafka broker (string)
    # kafka_port: kafka broker port (number)
    # msg_id: the id of the message to look for (str)
    inputs:
      parameters:
      - name: kafka_topic
      - name: kafka_ip
      - name: kafka_port
      - name: msg_id
    script:
      image: docker.pkg.github.com/5gzorro/issm/python:alpine3.6-kafka-v0.1
      imagePullPolicy: IfNotPresent
      command: [python]
      source: |
        import json
        import os
        import sys

        from kafka import KafkaConsumer
        from kafka.errors import KafkaError

        KAFKA_TOPIC = '{{inputs.parameters.kafka_topic}}'
        KAFKA_TIMEOUT = 10
        KAFKA_API_VERSION = (1, 1, 0)

        MSG_ID = "{{inputs.parameters.msg_id}}"
        sbi_ip = ""
        up_ip = ""

        KAFKA_SERVER = "{}:{}".format("{{inputs.parameters.kafka_ip}}", "{{inputs.parameters.kafka_port}}")
        consumer = KafkaConsumer(bootstrap_servers=KAFKA_SERVER,
                                 client_id="aaa",
                                 enable_auto_commit=True,
                                 api_version=KAFKA_API_VERSION, group_id='aaa-group-id')
        consumer.subscribe(pattern=KAFKA_TOPIC)

        for msg in consumer:
            # TODO: drain topic messages first so we do not consume stale messages
            payload = json.loads(msg.value.decode('utf-8', 'ignore'))
            sys.stdout.write('Received payload: %s \n' % payload)
            if payload['msg_id'] == MSG_ID:
                sys.stdout.write('It is my message: %s \n' % payload)
                sbi_ip = payload['sbi-ip']
                up_ip = payload['up-ip']
                break

        with open('/tmp/sbi-ip.txt', 'w') as f:
            f.write(sbi_ip)

        with open('/tmp/up-ip.txt', 'w') as f:
            f.write(up_ip)

    outputs:
      parameters:
      - name: sbi-ip
        valueFrom:
          path: /tmp/sbi-ip.txt

      - name: up-ip
        valueFrom:
          path: /tmp/up-ip.txt
